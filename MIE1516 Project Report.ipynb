{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Nets on Bank Loan Data to Minimize Defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When someone wants to apply for a loan at a financial institution, the applicant's payback ability and credit history are evaluated against the loan amount to see whether thte applicant is likely to pay back. If the applicant is likely to pay back, then the loan is granted. Otherwise, the loan would be rejected.\n",
    "\n",
    "Ever since the 2008 financial crisis, regulations on the financial industry has tightened (for a good cause). Banks stopped giving out loans to those that are unlikely to pay back (which is what they should be doing in the first place). This project intends on investigating the current methodologies that the banks are using, and see if we can use machine learning / deep learning approaches to beat the bank's current model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Bank's current model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is difficult to find any publicly available loan data from financial institutions. These datasets are typically highly confidential, as they contain sensitive personal information from the loan applicants. However, I managed to find the approved loan datasets from the peer-to-peer lending company, \"lending club\", from 2007 to 2015 (https://www.kaggle.com/wendykan/lending-club-loan-data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After removing ongoing loans, there are ~1.3 million samples. By classifying all problematic loans (late payments, called offs, defaults) as defaults, the bank's current model has a 78.38% precision. (There are no ways to know whether the rejected loans would default or not, if the applicant were given the loan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Preliminary Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets have many columns, but not all are useful. Some columns have repetitive features as others. For example, \"monthly installment\" is equivalent to some mathematical formula involving \"loan amount\" and \"interest rate\". There are also columns they are irrelevant to our analysis, such as \"next payment date\". After removing the extra columns, I ended up with these variables:\n",
    "    - Income: applicant annual income in USD.\n",
    "    - Verification: whether the applicant's self-declared income has been verified. Three categories for this data: \"Verified\", \"Source Verified\" (Verified that the applicant works at the self-declared company), and \"Not Verified\"\n",
    "    - Installment: the applicant's monthly payment.\n",
    "    - Term: the lengths of the loan period, in this case either \"36 months\" or \"60 months\"\n",
    "    - Homeownership: the collateral status, whether the applicant is the \"owner\", is \"morgaging\", or is \"renting\"\n",
    "    - Purpose: The reason for the loan, e.g. \"debt consolidation\", \"education\", etc.\n",
    "    - Location: The home state of the applicant, e.g. \"CA\", \"TX\", \"NY\", etc.\n",
    "    - Dti: Debt-to-Income ratio, ranges between 0 to 100.\n",
    "    - Delinq_2yrs: The number of times that the applicant becomes delinquent (late for payment) in the past 2 years\n",
    "    - Revov_util: The percentage of available credit that is being utilized. Usually ranges from 0 to 100, but can sometimes exceed 100 (unlikely in this dataset, since this type of applicant usually won't be granted loans)\n",
    "    - loan_status: the final outcome of the loan, either \"Fully Paid\" or \"Default\"\n",
    "    - grade: the loan grade that the lending club assigns to each loan. Loans with higher grade are more likely to be fully paid\n",
    "    - sub_grade: similar to grade, except that it's the sub_grade within each grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to use the features, [income, verification, installment, term, home_ownership, purpose, location, dti, delinq_2yrs, Revov_util] to predict [loan_status] using logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train_test_split is set at 70:30, and the train_test_split is done 10 times. The categorical values are converted to dummy nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>124</td>\n",
       "      <td>861929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137</td>\n",
       "      <td>3128630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0        1\n",
       "0  124   861929\n",
       "1  137  3128630"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "lr = pd.DataFrame([[124,861929],[137,3128630]])\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.7840079547752583\n",
      "accuracy: 0.7839877518905889\n"
     ]
    }
   ],
   "source": [
    "print('precision: {}'.format(lr[1][1]/(lr[1][0]+lr[1][1])))\n",
    "print('accuracy: {}'.format((lr[0][0]+lr[1][1])/(lr[0][0]+lr[0][1]+lr[1][0]+lr[1][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that there are some slight improvement in the model, although the improvement is negligible. Basically the logistic regression didn't find any patterns to classify the 'defaults' and 'fully paid' loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we have millions of datapoints, using SVM takes too much computation time, so we will go straight to the neural nets approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Fully Connected Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use neural net to train the features and see if there exist non-linear relationship among the features, which would produce a better classification than logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple neural nets are chosen for this particular dataset. The input categorical features are also converted to numeric values through one-hot encoding. I experiminented with the following structures:\n",
    "    - Number of Dense layers:\n",
    "        -1\n",
    "        -2\n",
    "        -3\n",
    "        -4\n",
    "    - Hidden layer type:\n",
    "        -sigmoid\n",
    "        -tanh\n",
    "        -relu\n",
    "        -elu\n",
    "        -selu\n",
    "    - Dimensionality of the output space of each layer (256, 128, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All settings with sigmoid and tanh activations produce accuracy of around 78.46% (this depends on the train_test_split randomizer). Relu, elu, and selu had weird performances where all loans are marked as \"Defaults\", which gave very low accuracies of 21.66%. The accuracies usually converge after 1-2 epoch. This shows that the FCN also failed to distinguish the \"Defaults\" and \"Fully Paid\" loans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activation</th>\n",
       "      <th>dim=256</th>\n",
       "      <th>dim=128</th>\n",
       "      <th>dim=64</th>\n",
       "      <th>dim=32</th>\n",
       "      <th>dim=16</th>\n",
       "      <th>dim=8</th>\n",
       "      <th>dim=4</th>\n",
       "      <th>dim=2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elu</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.7843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>selu</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Activation  dim=256  dim=128  dim=64  dim=32  dim=16   dim=8   dim=4   dim=2\n",
       "0    sigmoid   0.7843   0.7843  0.7843  0.7843  0.7843  0.7843  0.7843  0.7843\n",
       "1       tanh   0.7843   0.7843  0.7843  0.7843  0.7843  0.7843  0.7843  0.7843\n",
       "2       relu   0.2157   0.2157  0.2157  0.2157  0.2157  0.2157  0.2157  0.2157\n",
       "3        elu   0.2157   0.2157  0.2157  0.2157  0.2157  0.2157  0.2157  0.7843\n",
       "4       selu   0.2157   0.2157  0.2157  0.2157  0.2157  0.7843  0.7843  0.7843"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1data = [['sigmoid', 0.7843, 0.7843, 0.7843, 0.7843, 0.7843, 0.7843, 0.7843, 0.7843], ['tanh', 0.7843, 0.7843, 0.7843, 0.7843, 0.7843, 0.7843, 0.7843, 0.7843], ['relu', 0.2157, 0.2157, 0.2157, 0.2157, 0.2157, 0.2157, 0.2157, 0.2157],['elu', 0.2157, 0.2157, 0.2157, 0.2157, 0.2157, 0.2157, 0.2157, 0.7843],['selu', 0.2157, 0.2157, 0.2157, 0.2157, 0.2157, 0.7843, 0.7843, 0.7843]]\n",
    "l1 = pd.DataFrame(layer1data, columns = ['Activation', 'dim=256', 'dim=128', 'dim=64','dim=32','dim=16', 'dim=8','dim=4','dim=2'])\n",
    "l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activation</th>\n",
       "      <th>dim=256</th>\n",
       "      <th>dim=128</th>\n",
       "      <th>dim=64</th>\n",
       "      <th>dim=32</th>\n",
       "      <th>dim=16</th>\n",
       "      <th>dim=8</th>\n",
       "      <th>dim=4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elu</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.7843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>selu</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.7843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Activation  dim=256  dim=128  dim=64  dim=32  dim=16   dim=8   dim=4\n",
       "0    sigmoid   0.7843   0.7843  0.7843  0.7843  0.7843  0.7843  0.7843\n",
       "1       tanh   0.7843   0.7843  0.7843  0.7843  0.7843  0.7843  0.7843\n",
       "2       relu   0.2157   0.2157  0.2157  0.2157  0.2157  0.2157  0.2157\n",
       "3        elu   0.2157   0.2157  0.2157  0.2157  0.2157  0.2157  0.7843\n",
       "4       selu   0.2157   0.2157  0.2157  0.2157  0.2157  0.2157  0.7843"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer2data = [['sigmoid', 0.7843, 0.7843, 0.7843, 0.7843, 0.7843, 0.7843, 0.7843], ['tanh', 0.7843, 0.7843, 0.7843, 0.7843, 0.7843, 0.7843, 0.7843], ['relu', 0.2157, 0.2157, 0.2157, 0.2157, 0.2157, 0.2157, 0.2157],['elu', 0.2157, 0.2157, 0.2157, 0.2157, 0.2157, 0.2157, 0.7843],['selu', 0.2157, 0.2157, 0.2157, 0.2157, 0.2157, 0.2157, 0.7843]]\n",
    "l2 = pd.DataFrame(layer2data, columns = ['Activation', 'dim=256', 'dim=128', 'dim=64','dim=32','dim=16', 'dim=8','dim=4'])\n",
    "l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activation</th>\n",
       "      <th>dim=256</th>\n",
       "      <th>dim=128</th>\n",
       "      <th>dim=64</th>\n",
       "      <th>dim=32</th>\n",
       "      <th>dim=16</th>\n",
       "      <th>dim=8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elu</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>selu</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Activation  dim=256  dim=128  dim=64  dim=32  dim=16   dim=8\n",
       "0    sigmoid   0.7843   0.7843  0.7843  0.7843  0.7843  0.7843\n",
       "1       tanh   0.7843   0.7843  0.7843  0.7843  0.7843  0.7843\n",
       "2       relu   0.2157   0.2157  0.2157  0.2157  0.2157  0.2157\n",
       "3        elu   0.2157   0.2157  0.2157  0.2157  0.2157  0.2157\n",
       "4       selu   0.2157   0.2157  0.2157  0.2157  0.2157  0.2157"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer3data = [['sigmoid', 0.7843, 0.7843, 0.7843, 0.7843, 0.7843, 0.7843], ['tanh', 0.7843, 0.7843, 0.7843, 0.7843, 0.7843, 0.7843], ['relu', 0.2157, 0.2157, 0.2157, 0.2157, 0.2157, 0.2157],['elu', 0.2157, 0.2157, 0.2157, 0.2157, 0.2157, 0.2157],['selu', 0.2157, 0.2157, 0.2157, 0.2157, 0.2157, 0.2157]]\n",
    "l3 = pd.DataFrame(layer3data, columns = ['Activation', 'dim=256', 'dim=128', 'dim=64','dim=32','dim=16', 'dim=8'])\n",
    "l3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activation</th>\n",
       "      <th>dim=256</th>\n",
       "      <th>dim=128</th>\n",
       "      <th>dim=64</th>\n",
       "      <th>dim=32</th>\n",
       "      <th>dim=16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.7843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elu</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.7843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>selu</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.2157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Activation  dim=256  dim=128  dim=64  dim=32  dim=16\n",
       "0    sigmoid   0.7843   0.7843  0.7843  0.7843  0.7843\n",
       "1       tanh   0.7843   0.7843  0.7843  0.7843  0.7843\n",
       "2       relu   0.2157   0.2157  0.2157  0.2157  0.2157\n",
       "3        elu   0.2157   0.2157  0.2157  0.2157  0.7843\n",
       "4       selu   0.2157   0.2157  0.2157  0.7843  0.2157"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer4data = [['sigmoid', 0.7843, 0.7843, 0.7843, 0.7843, 0.7843], ['tanh', 0.7843, 0.7843, 0.7843, 0.7843, 0.7843], ['relu', 0.2157, 0.2157, 0.2157, 0.2157, 0.2157],['elu', 0.2157, 0.2157, 0.2157, 0.2157, 0.7843],['selu', 0.2157, 0.2157, 0.2157, 0.7843, 0.2157]]\n",
    "l4 = pd.DataFrame(layer4data, columns = ['Activation', 'dim=256', 'dim=128', 'dim=64','dim=32','dim=16'])\n",
    "l4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Logistic Regression + Autoencoder trained categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the first 2 models failed to beat the bank's current model, let's see if we can improve the results by training the categorical data to discover the hidden relationship between each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of training autoencoder for categorical data is taken from this blog: https://medium.com/@satnalikamayank12/on-learning-embeddings-for-categorical-data-using-keras-165ff2773fc9\n",
    "\n",
    "Several depreciated keras methodology was reconstructued. Multiple structures were tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>116</td>\n",
       "      <td>862473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>136</td>\n",
       "      <td>3128095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0        1\n",
       "0  116   862473\n",
       "1  136  3128095"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae = pd.DataFrame([[116,862473],[136,3128095]])\n",
    "ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.7838721204600447\n",
      "accuracy: 0.7838516896276956\n"
     ]
    }
   ],
   "source": [
    "print('precision: {}'.format(ae[1][1]/(ae[1][0]+ae[1][1])))\n",
    "print('accuracy: {}'.format((ae[0][0]+ae[1][1])/(ae[0][0]+ae[0][1]+ae[1][0]+ae[1][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the results did not improve... No matter what the output structure of the autoencoder is chosen, the classifier always classify everything as \"Fully Paid. This leads to the conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After testing the dataset with these three models, I have reached a clear conclusion: among the loans that are approved by banks, the chance of the loaner to go default is COMPLETELY RANDOM. Regardless of the income level and credit history, every applicant has a not-so-insignificant chance of defaulting. In other word, the existing bank model (no matter what they're using) has already been optimized, and the current bank model is able to identify the high-risk applicants and reject them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6- Some Additional Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of predicting whether the loan is going to default or not, the bank assigns each loan a grade and a subgrade, much like how credit rating agencies S&P or Moody assigns bonds with a alphebetical rating (\"A\", \"B\", \"C\", etc.). However, this is a tricky process to model, since loans with lower grades would results in a higher interest rate, which would increase the chance of loan defaults (this would create a cycle, violating the DAG assumption in many of the models).\n",
    "Nevertheless, the bank's existing system on loan grades is quite accurate in giving out loan grades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payback_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grade</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.932625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.853035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.756580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.675571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>0.597655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>0.533508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>0.513876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       payback_rate\n",
       "grade              \n",
       "A          0.932625\n",
       "B          0.853035\n",
       "C          0.756580\n",
       "D          0.675571\n",
       "E          0.597655\n",
       "F          0.533508\n",
       "G          0.513876"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_df = pd.DataFrame(data = {'grade':['E', 'D', 'B', 'F', 'C', 'A', 'G'],'payback_rate':[0.5976554234203733, 0.675571154819941, 0.8530354337037632, 0.5335076923076923, 0.7565798326644662, 0.9326246273146014, 0.5138755980861244]}).set_index('grade').sort_index()\n",
    "gd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payback_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub_grade</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A1</th>\n",
       "      <td>0.963330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2</th>\n",
       "      <td>0.947411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3</th>\n",
       "      <td>0.937556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A4</th>\n",
       "      <td>0.923349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A5</th>\n",
       "      <td>0.908449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B1</th>\n",
       "      <td>0.884263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B2</th>\n",
       "      <td>0.875420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B3</th>\n",
       "      <td>0.858656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B4</th>\n",
       "      <td>0.837858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B5</th>\n",
       "      <td>0.815937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C1</th>\n",
       "      <td>0.794084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C2</th>\n",
       "      <td>0.775330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C3</th>\n",
       "      <td>0.756062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C4</th>\n",
       "      <td>0.729700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C5</th>\n",
       "      <td>0.717887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D1</th>\n",
       "      <td>0.702889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2</th>\n",
       "      <td>0.683880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3</th>\n",
       "      <td>0.672963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D4</th>\n",
       "      <td>0.655619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D5</th>\n",
       "      <td>0.643881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E1</th>\n",
       "      <td>0.623495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E2</th>\n",
       "      <td>0.609566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E3</th>\n",
       "      <td>0.593987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E4</th>\n",
       "      <td>0.581476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E5</th>\n",
       "      <td>0.560395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.561905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F2</th>\n",
       "      <td>0.533892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F3</th>\n",
       "      <td>0.535211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F4</th>\n",
       "      <td>0.506362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F5</th>\n",
       "      <td>0.507589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G1</th>\n",
       "      <td>0.502628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G2</th>\n",
       "      <td>0.501606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G3</th>\n",
       "      <td>0.522659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G4</th>\n",
       "      <td>0.529104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G5</th>\n",
       "      <td>0.549284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           payback_rate\n",
       "sub_grade              \n",
       "A1             0.963330\n",
       "A2             0.947411\n",
       "A3             0.937556\n",
       "A4             0.923349\n",
       "A5             0.908449\n",
       "B1             0.884263\n",
       "B2             0.875420\n",
       "B3             0.858656\n",
       "B4             0.837858\n",
       "B5             0.815937\n",
       "C1             0.794084\n",
       "C2             0.775330\n",
       "C3             0.756062\n",
       "C4             0.729700\n",
       "C5             0.717887\n",
       "D1             0.702889\n",
       "D2             0.683880\n",
       "D3             0.672963\n",
       "D4             0.655619\n",
       "D5             0.643881\n",
       "E1             0.623495\n",
       "E2             0.609566\n",
       "E3             0.593987\n",
       "E4             0.581476\n",
       "E5             0.560395\n",
       "F1             0.561905\n",
       "F2             0.533892\n",
       "F3             0.535211\n",
       "F4             0.506362\n",
       "F5             0.507589\n",
       "G1             0.502628\n",
       "G2             0.501606\n",
       "G3             0.522659\n",
       "G4             0.529104\n",
       "G5             0.549284"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_df = pd.DataFrame({'sub_grade':['B2', 'G4', 'A1', 'C5', 'F2', 'F3', 'G2', 'E1', 'D5', 'A5', 'E2', 'C3', 'F5', 'C4', 'B1', 'F4', 'D2', 'G5', 'E5', 'C1', 'D1', 'A4', 'B5', 'G3', 'D4', 'C2', 'E3', 'G1', 'F1', 'A3', 'B4', 'D3', 'A2', 'E4', 'B3'],'payback_rate':[0.8754203649594796, 0.5291044776119403, 0.9633297903179051, 0.7178874577076038, 0.5338924790320363, 0.5352112676056338, 0.5016062413951354, 0.6234953655160844, 0.6438809626270316, 0.9084486781958252, 0.6095658230787091, 0.7560621928623218, 0.5075889524757402, 0.7296998462824628, 0.8842627795870934, 0.5063623510401939, 0.6838801903017578, 0.5492839090143218, 0.5603946920721333, 0.7940838632515068, 0.7028887978621725, 0.923348778763842, 0.815936706676854, 0.5226586102719033, 0.6556187487742695, 0.7753298354643167, 0.5939873929206401, 0.5026281208935611, 0.5619047619047619, 0.9375559716681593, 0.8378576489847778, 0.6729629817187421, 0.947410823424825, 0.5814758846492885, 0.8586563211101578]}).set_index('sub_grade').sort_index()\n",
    "sgd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there exists a clear hierachy in the percentage of \"Fully Paid\" loans within the loan grade and subgrades. We can see that there are some mismatches at the lowest sub-grade levels, particularlly the \"G\" grade. However, it should be noted that the \"G\" grade loans only make up a small percentage of the whole dataset (<0.08% for each sub-grade). Our classification models managed to identify a small negligible amount of bad loans, which would most likely be the ones from the \"G\" grade loans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7- Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenges that are encountered in this project can be summarized in the following few points:\n",
    "    1. The target bank model to beat is an optimized one, and this is discovered after building all the models.\n",
    "    2. The nature of the data made it impossible to do a regression instead of a \"Paid\" vs \"Default\" classification. The only indication of Default rate is the \"grade\" and \"sub-grade\" that the bank assigns to each loan, which is subjective rather than objective. \n",
    "    3. Even if we could train a 100% accurate regression model that reflects the default rate of each loan, it is equivalent to reproducing the bank's existing grading model, which is useless to any interested stakeholder.\n",
    "    4. There were some paper that suggest that banks use a graphical model approach towards loan default rate. Upon further review, these would most likely be corporate loans, since some of the data regarding corporate loans are unmeasurable/unreportable (e.g. industry outlook, corporate outlook). However, banks require complete information for personal loans, which means the graphical model approach would not be effective (no need for inferences, just train P(Default|features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, future work should be focused on acquiring a complete dataset, which includes both approved and rejected loan data (and assume that all rejected loan data will be defaults), to see how the each feature relate to one another in causing the default. On the other hand, as regulations are tightening since the financial crisis, we can see that the existing loan evaluation system implemented at the financial institutions are quite mature. In conclusion, banks have become a lot more responsible in terms of issuing loans since the subprime mortgage crisis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:2019_tf2_env]",
   "language": "python",
   "name": "conda-env-2019_tf2_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
